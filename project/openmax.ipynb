{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impot\n",
    "import os\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78b143",
   "metadata": {},
   "source": [
    "## Classifier model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ffb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 경로 수정 필요\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/dt/data'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "VAL_PATH = os.path.join(DATA_PATH, 'val')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 생성 함수\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    \n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    val_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224), interpolation=nearest_mode),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "       \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "    \n",
    "    print(len(data))\n",
    "    \n",
    "    return dataloader, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 설정 \n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# TTRAIN_PATH에 들어있는 데이터를 shuffle 하도록 dataloader 만들기\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1785041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 별 이미지 개수 (TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
    "for dirpath, dirnames, filenames in os.walk(TRAIN_PATH):\n",
    "    print(f'{dirpath} : {len(filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcb2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VAL loder 생성\n",
    "val_loader, _val_data = create_dataloader(VAL_PATH, BATCH_SIZE, False)\n",
    "print('val: ', _val_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b341cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST loder 생성\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, BATCH_SIZE, False)\n",
    "print('test: ', _test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964424ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"데이터셋 샘플 수: {len(_train_data)}\")\n",
    "print(f\"데이터로더 배치 수: {len(train_loader)}\")\n",
    "print(f\"현재 DataLoader의 배치 크기: {train_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics 함수\n",
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f000cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 함수\n",
    "# 1 epoch 당 1회 수행되는 train 함수\n",
    "# optimizer : Adam, loss 함수 : CrossEntropyLoss\n",
    "def train(dataloader, net, learning_rate, weight_decay_level, device):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr = learning_rate, \n",
    "        weight_decay = weight_decay_level\n",
    "    )\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    train_losses = list()\n",
    "    train_preds = list()\n",
    "    train_trues = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # gradient 초기화\n",
    "\n",
    "        out = net(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        loss.backward() # gradient 계산\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        train_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(train_trues, train_preds)\n",
    "\n",
    "    print('\\n''====== Training Metrics ======')\n",
    "    print('Loss:', mean(train_losses), 'Acc:', acc, 'F1:', f1, 'Precision:', prec, 'Recall:', rec)\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 함수\n",
    "def test(dataloader, net, device):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out = net(img)\n",
    "\n",
    "            _, pred = torch.max(out, 1)\n",
    "            loss = criterion(out, label)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "            test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "    acc, f1, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "\n",
    "    print('====== Test Metrics ======')\n",
    "    print('Loss:', mean(test_losses), 'Acc:', acc, 'F1:', f1, 'Precision:', prec, 'Recall:', rec)\n",
    "\n",
    "    return net, acc, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaaab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 함수( 폴더생성, 훈련, 평가, high-acc pth 저장)\n",
    "def train_classifier(net, train_loader, val_loader, n_epochs, learning_rate, weight_decay, device):\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    model_save_path = None\n",
    "    model_save_base = 'weights'\n",
    "    if not os.path.exists(model_save_base):\n",
    "        os.makedirs(model_save_base)\n",
    "    \n",
    "    print('>> Start Training Model!')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print('> epoch: ', epoch)\n",
    "\n",
    "        net, _, _, _, _ = train(train_loader, net, learning_rate, weight_decay, device)\n",
    "        net, test_acc, _, _, _  = test(val_loader, net, device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "\n",
    "            best_test_acc = test_acc\n",
    "            test_acc_str = '%.5f' % test_acc\n",
    "\n",
    "            print('[Notification] Best Model Updated!')\n",
    "            model_save_path = os.path.join(model_save_base, 'classifier_acc_' + str(test_acc_str) + '.pth') \n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "                \n",
    "    return model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e73068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resnet50 model\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5cc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch, lr, weight 설정 후 학습 시작\n",
    "EPOCHS = 80\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 0.001\n",
    "\n",
    "saved_weight_path = train_classifier(net, train_loader, val_loader, EPOCHS, LEARNING_RATE, WEIGHT_DECAY, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedb4ab",
   "metadata": {},
   "source": [
    "## open max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f04420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습한 모델 로드 하여 Eval()\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "saved_weight_path = '../classifier_weights.pth'\n",
    "net.load_state_dict(torch.load(saved_weight_path, map_location=device))\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 설정 \n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# TTRAIN_PATH에 들어있는 데이터를 shuffle 하도록 dataloader 만들기\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, BATCH_SIZE, True)\n",
    "target_class_num = len(os.listdir(os.path.join(TRAIN_PATH)))\n",
    "\n",
    "print('target_class_num: ', target_class_num)\n",
    "print('train: ', _train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906dae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenMax에 사용할 데이터를 추출\n",
    "train_preds = list()\n",
    "train_actvecs = list()\n",
    "train_outputs_softmax = list()\n",
    "train_labels = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (img, label) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        out = net(img)\n",
    "        out_actvec = out.cpu().detach().numpy()[0]\n",
    "        out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "        out_pred = int(torch.argmax(out).cpu().detach().numpy())\n",
    "        out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "        train_actvecs.append(out_actvec) # component 1: softmax 전의 Activation Vector\n",
    "        train_preds.append(out_pred) # componenet 2: 각 데이터에 대한 예측값\n",
    "        train_outputs_softmax.append(out_softmax) # component 3: 각 데이터에 대한 softmax 확률\n",
    "        train_labels.append(out_label) # component 4: 각 데이터에 대한 Label (정답)\n",
    "\n",
    "train_actvecs = np.asarray(train_actvecs)\n",
    "train_preds = np.asarray(train_preds)\n",
    "train_outputs_softmax = np.asarray(train_outputs_softmax)\n",
    "train_labels = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올바른 경우의 Activation Vector만 사용\n",
    "train_correct_actvecs = train_actvecs[train_labels==train_preds]\n",
    "train_correct_labels = train_labels[train_labels==train_preds]\n",
    "\n",
    "print('Activation vector: ', train_correct_actvecs.shape)\n",
    "print('Labels: ', train_correct_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Vector를 클래스마다 나눠 담기\n",
    "# 클래스별로 나눠진 Activation Vector별 평균으로부터 가장 먼 100개의 Vector를 이용해 베이불 분포의 모수를 추출\n",
    "# 각 클래스당 베이불 분포의 모수들을 저장\n",
    "class_means = list()\n",
    "dist_to_means = list()\n",
    "mr_models = {}\n",
    "\n",
    "for class_idx in np.unique(train_labels):\n",
    "    \n",
    "    class_act_vec = train_correct_actvecs[train_correct_labels==class_idx]\n",
    "    print('class_idx: ', class_idx)\n",
    "    print(class_act_vec.shape)\n",
    "    \n",
    "    class_mean = class_act_vec.mean(axis=0)\n",
    "    class_means.append(class_mean)\n",
    "    \n",
    "    dist_to_mean = np.square(class_act_vec - class_mean).sum(axis=1) # 각 activation vector의 거리를 계산\n",
    "    dist_to_mean_sorted = np.sort(dist_to_mean).astype(np.float64) # 거리를 기준으로 오름차순 정렬\n",
    "    dist_to_means.append(dist_to_mean_sorted)\n",
    "\n",
    "    shape, loc, scale = stats.weibull_max.fit(dist_to_mean_sorted[-100:]) # 거리가 가장 먼 100개를 사용하여 모수 추출\n",
    "    \n",
    "    mr_models[str(class_idx)] = {\n",
    "        'shape':shape,\n",
    "        'loc':loc,\n",
    "        'scale':scale\n",
    "    }\n",
    "    \n",
    "class_means = np.asarray(class_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenMax 확률 함수\n",
    "def compute_openmax(actvec, class_means, mr_models):\n",
    "    dist_to_mean = np.square(actvec - class_means).sum(axis=1)\n",
    "\n",
    "    scores = list()\n",
    "    for class_idx in range(len(class_means)):\n",
    "        params = mr_models[str(class_idx)]\n",
    "        score = stats.weibull_max.cdf(\n",
    "            dist_to_mean[class_idx],\n",
    "            params['shape'],\n",
    "            params['loc'],\n",
    "            params['scale']\n",
    "        )\n",
    "        scores.append(score)\n",
    "    scores = np.asarray(scores)\n",
    "    \n",
    "    weight_on_actvec = 1 - scores # 각 class별 가중치\n",
    "    rev_actvec = np.concatenate([\n",
    "        weight_on_actvec * actvec, # known class에 대한 가중치 곱\n",
    "        [((1-weight_on_actvec) * actvec).sum()] # unknown class에 새로운 계산식\n",
    "    ])\n",
    "    \n",
    "    openmax_prob = np.exp(rev_actvec) / np.exp(rev_actvec).sum()\n",
    "    return openmax_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산한 최대 확률이 threshold 보다 낮은 경우라면 강제로 reject클래스로 분류해주는 함수\n",
    "def inference(actvec, threshold, target_class_num, class_means, mr_models):\n",
    "    openmax_prob = compute_openmax(actvec, class_means, mr_models)\n",
    "    openmax_softmax = np.exp(openmax_prob)/sum(np.exp(openmax_prob))\n",
    "    pred = np.argmax(openmax_softmax)\n",
    "\n",
    "    if np.max(openmax_softmax) < threshold:\n",
    "        pred = target_class_num\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24932c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold 탐색을 쉽게 하기 위한 함수\n",
    "def inference_dataloader(net, data_loader, threshold, target_class_num, class_means, mr_models, is_reject=False):\n",
    "    result_preds = list()\n",
    "    result_labels = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, label) in enumerate(data_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            out = net(img)\n",
    "            out_actvec = out.cpu().detach().numpy()[0]\n",
    "            out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "            out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "            pred = inference(out_actvec, threshold, target_class_num, class_means, mr_models)\n",
    "            result_preds.append(pred)\n",
    "        \n",
    "            if is_reject:\n",
    "                result_labels.append(target_class_num)\n",
    "            else:\n",
    "                result_labels.append(out_label)\n",
    "\n",
    "    return result_preds, result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454901c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, reject dataloader 생성\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, 1, False)\n",
    "reject_loader, _reject_data = create_dataloader(REJECT_PATH, 1, False)\n",
    "target_class_num = len(os.listdir(TEST_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 threshold test \n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.45, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.45, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2102e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 threshold test \n",
    "thresholds = np.arange(0.36, 0.50, 0.02)\n",
    "test_accuracies = list()\n",
    "reject_accuracies = list()\n",
    "\n",
    "for idx, threshold in enumerate(thresholds):\n",
    "    test_preds, test_labels = inference_dataloader(net, test_loader, threshold, target_class_num, class_means, mr_models)\n",
    "    reject_preds, reject_labels = inference_dataloader(net, reject_loader, threshold, target_class_num, class_means, mr_models, is_reject=True)\n",
    "    \n",
    "    test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "    reject_accuracy = accuracy_score(reject_labels, reject_preds)\n",
    "    \n",
    "    test_accuracies.append(test_accuracy)\n",
    "    reject_accuracies.append(reject_accuracy)\n",
    "\n",
    "# np.array 형 변환\n",
    "test_accuracies = np.asarray(test_accuracies)\n",
    "reject_accuracies = np.asarray(reject_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_score의 값 이상일 때 reject_accuracy가 최대가 되는 threashold 찾기\n",
    "target_score = 0.85\n",
    "best_reject_accuracy = 0\n",
    "best_test_accuracy = None\n",
    "best_threashold = None\n",
    "for idx, flag in enumerate(test_accuracies > target_score):\n",
    "    if flag == True and best_reject_accuracy < reject_accuracies[idx]:\n",
    "        best_threshold = thresholds[idx]\n",
    "        best_test_accuracy = test_accuracies[idx]\n",
    "        best_reject_accuracy = reject_accuracies[idx]\n",
    "\n",
    "print(f\"Test accuracy가 {target_score} 이상일 때: \")\n",
    "print(f\"reject accuracy의 최대값: {best_reject_accuracy}\")\n",
    "print(f\"Test accuracy 값: {best_test_accuracy}\")\n",
    "print(f\"threshold 값: {round(best_threshold,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, Reject Accuracy 시각화\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(thresholds, test_accuracies, label='Test Accuracy', marker='o')\n",
    "plt.plot(thresholds, reject_accuracies, label='Reject Accuracy', marker='x')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy & Reject Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('./openmax_threshold.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecaa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
